{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29837f9",
   "metadata": {},
   "source": [
    "Welcome hopefully prospective employers!\n",
    "\n",
    "Unlike LOKI, I'm doing this project exclusively to demonstrate my coding and commenting styles in a finished ML project.\n",
    "\n",
    "\n",
    "So my goal here is to build a quick and dirty ML model to predict toxicity of biological compounds. The dataset is awesome, it's the ToxCast dataset from moleculenet.org, but it also only has 600 data points in it which may be too few for a complex model like this. I'm going to try to add dropout layers to maximize data but in the end it's more of a showcase than a solution, I have my other project that gets that kind of attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c65a7-2fcf-4345-a66d-1b3ee53b77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "\n",
    "# Loading toxcast data (pandas df)\n",
    "toxcast_df = pd.read_csv('toxcast_data.csv')\n",
    "\n",
    "# Selecting relevant columns (all labels excluding SMILES)\n",
    "labels = toxcast_df.columns[1:].tolist()\n",
    "\n",
    "# Filling missing values with 0 and making them integers\n",
    "toxcast_df[labels] = toxcast_df[labels].fillna(0).astype(int)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "smiles = toxcast_df['smiles'].values\n",
    "y = toxcast_df[labels].values\n",
    "\n",
    "#checking out our new arrays\n",
    "print(f\"y shape is {y.shape}\")\n",
    "print(y[0:10], 0)\n",
    "print(f\"smiles shae is:{smiles.shape}\")\n",
    "print(smiles[0:2], 0)\n",
    "\n",
    "#Take a look at the difference in quantity of 0 and 1. There's a full order of magnitude difference\n",
    "plt.figure(figsize=(8, 6))\n",
    "counts = [np.sum(y == 0), np.sum(y == 1)]\n",
    "bars = plt.bar(['0s', '1s'], counts)\n",
    "plt.title('Counts of 0s and 1s in y array')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(), count, \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625681-d273-43ff-93b8-beb343cba58c",
   "metadata": {},
   "source": [
    "Time to tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d972a62-46a6-46aa-863d-154c9db5fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_overall_toxicity = (y.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(smiles)\n",
    "sequences = tokenizer.texts_to_sequences(smiles)\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "X = np.array(X)\n",
    "#y = np.array(y)\n",
    "y_overall_toxicity = np.array(y_overall_toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b49b4a-9a75-40ba-a3d6-fff094ba107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_overall_toxicity, test_size=0.2, random_state=42)\n",
    "\n",
    "a = np.random.randint(0, 1500)\n",
    "print(a)\n",
    "print(smiles[a])\n",
    "print(f\"Sample tokenized SMILES: {sequences[a]}\")\n",
    "print(f\"Padded tokenized SMILES: {X[a]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8f07b-d2f6-4508-aa5a-a742f159e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#callback = callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703a27a-89cc-45ed-a249-251301701e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2)#, callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89215324-183a-4eff-8259-fb7108ac250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387d752-e1d9-4e4d-a383-dd1185163fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82f3d8-2e90-4334-94d7-8ee881ae446b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
